{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizedQuery\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "import os\n",
    "import dotenv\n",
    "import base64\n",
    "import re\n",
    "from azure.ai.documentintelligence.models import AnalyzeDocumentRequest, DocumentContentFormat, AnalyzeOutputOption\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "dotenv.load_dotenv(override=True)\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import glob\n",
    "import json\n",
    "import hashlib\n",
    "import uuid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = AzureOpenAI(\n",
    "    api_key=os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "di_client : DocumentIntelligenceClient = DocumentIntelligenceClient(\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_API_KEY\")),\n",
    "    endpoint=os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aiseach_client = SearchClient(\n",
    "    endpoint=os.getenv(\"AZURE_AISEARCH_ENDPOINT\"),\n",
    "    index_name=os.getenv(\"AZURE_AISEARCH_INDEX_NAME\"),\n",
    "    credential=AzureKeyCredential(os.getenv(\"AZURE_AISEARCH_KEY\"))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Location where the source files to be read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "source_path = os.path.join(os.getcwd(),\"src\") \n",
    "output_raw_document_path = os.path.join(os.getcwd(), \"output_raw_documents\")\n",
    "destination_folder = \"output_raw_documents\"\n",
    "\n",
    "if not os.path.exists(source_path):\n",
    "    os.makedirs(source_path)\n",
    "if not os.path.exists(output_raw_document_path):\n",
    "    os.makedirs(output_raw_document_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def list_files_glob(path, recursive=True):\n",
    "    pattern=f\"{path}/**/*.pdf\"\n",
    "    files = glob.glob(pattern, recursive=recursive)\n",
    "    list_of_files = []\n",
    "    for file in files:\n",
    "        if isfile(file):\n",
    "            list_of_files.append({\n",
    "                \"file_name\": file.split(\"\\\\\")[-1],\n",
    "                \"relative_path\": file.split(source_path)[-1][1:],\n",
    "                \"absolute_path\": file,\n",
    "                \"path_wo_origin\": path_wo_origin(file),\n",
    "                \"name_wo_extension\": file.split(\"\\\\\")[-1].split(\".\")[0]\n",
    "            })\n",
    "    return list_of_files\n",
    "\n",
    "def path_wo_origin(path):\n",
    "    filename = path.split(\"\\\\\")[-1]\n",
    "    path_wo_origin = path.split(filename)[0].split(source_path)[-1]\n",
    "    path_wo_origin = path_wo_origin[1:]\n",
    "    len_path = len(path_wo_origin) - 1\n",
    "    path_wo_origin = path_wo_origin[:len_path]\n",
    "    return path_wo_origin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_image_from_local(image_bytes: str) -> str:\n",
    "    \n",
    "    base64_image = base64.b64encode(image_bytes).decode('utf-8')\n",
    "    \n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o',\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": \"You are a helpful assistant to analyse images.\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [\n",
    "                    {\"type\": \"text\", \"text\": \"Please provide a detailed analysis of the image, including any relevant context or information that can be inferred from it. \\\n",
    "                     Respond in an objective non sexual, violenting or self-threating manner. Avoid violenting phrases.\"},\n",
    "                    {\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/png;base64,{base64_image}\"}\n",
    "                    },\n",
    "                ],\n",
    "            },\n",
    "        ],\n",
    "        max_tokens=2000,\n",
    "        temperature=0.0,\n",
    "    )\n",
    "\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_document(file_bytes: bytes) -> dict:\n",
    "    di_poller = di_client.begin_analyze_document(\n",
    "    \"prebuilt-layout\",\n",
    "    AnalyzeDocumentRequest(bytes_source=file_bytes),\n",
    "    output_content_format=DocumentContentFormat.MARKDOWN,\n",
    "    output=[AnalyzeOutputOption.FIGURES])\n",
    "\n",
    "    di_result = di_poller.result()\n",
    "    \n",
    "    return {\n",
    "        \"model_id\": di_result.model_id,\n",
    "        \"operation_id\": di_poller.details.get('operation_id'),\n",
    "        \"result\": di_result.as_dict()\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_output_directory(file_name):\n",
    "    if not os.path.exists(file_name):\n",
    "        os.makedirs(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_images_from_analyzed_document(analyzed_document: dict, file):\n",
    "    output_path=os.path.join(output_raw_document_path,file.get(\"path_wo_origin\"),file.get(\"name_wo_extension\"))\n",
    "    create_output_directory(output_path)\n",
    "    \n",
    "    operations_id = analyzed_document.get(\"operation_id\")\n",
    "    model_id = analyzed_document.get(\"model_id\")\n",
    "\n",
    "    if analyzed_document.get(\"result\"):\n",
    "        if analyzed_document.get(\"result\").get(\"figures\"):\n",
    "            image_ids_to_download = [figure[\"id\"] for figure in analyzed_document[\"result\"][\"figures\"]]\n",
    "            image_pathes = []\n",
    "            if operations_id and model_id:\n",
    "                for image_id in image_ids_to_download:\n",
    "                    img_bytes = di_client.get_analyze_result_figure(model_id, operations_id, image_id)\n",
    "                    image_id = image_id.replace(\".\", \"_\")\n",
    "                    with open(os.path.join(output_path,image_id+\".png\"), \"wb\") as image_file:\n",
    "                        image_file.writelines(img_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_image_description(filepath: str):\n",
    "    with open(filepath, \"rb\") as image_file:\n",
    "        image_bytes = image_file.read()\n",
    "        return analyze_image_from_local(image_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_image_with_generated_text(analyzed_document):\n",
    "    try:\n",
    "        figures_per_page = []\n",
    "        list_of_figures = analyzed_document.get(\"result\", {}).get(\"figures\", [])\n",
    "        analyzed_images = analyzed_document.get(\"analyzed_images\", [])\n",
    "        split_document = analyzed_document.get(\"split_document\", [])\n",
    "        paragraphs = analyzed_document.get(\"result\", {}).get(\"paragraphs\", [])\n",
    "\n",
    "        edited_pages = []\n",
    "        for page in split_document:\n",
    "            page_number = int(page.get(\"pageNumber\"))\n",
    "            page_content = page.get(\"pageContent\")\n",
    "            edited_page_content = page_content\n",
    "\n",
    "            figures_per_page = [figure for figure in list_of_figures if figure.get(\"boundingRegions\", [{}])[0].get(\"pageNumber\") == page_number]\n",
    "\n",
    "            for figure in figures_per_page:\n",
    "                analyzed_image_text = None\n",
    "                for image in analyzed_images:\n",
    "                    if image.get(\"name\") == figure.get(\"id\").replace(\".\", \"_\"):\n",
    "                        analyzed_image_text = image.get(\"image_description\")\n",
    "                        break\n",
    "\n",
    "                if analyzed_image_text:\n",
    "                    if figure.get(\"elements\"):\n",
    "                        ids = [int(element.split(\"/\")[-1]) for element in figure.get(\"elements\")]\n",
    "                        replace_list = [paragraphs[idx].get(\"content\").replace(\":selected: \",\"\").replace(\":unselected:\",\"\").replace(\":unselected: \",\"\").replace(\":selected:\", \"\").strip() for idx in ids if idx < len(paragraphs)]\n",
    "                        replace_list = [item for item in replace_list if item != '']\n",
    "                        if replace_list:\n",
    "                            pattern = r\"(\" + r\"\\s+\".join(map(re.escape, replace_list)) + r\")\"\n",
    "                            edited_page_content = re.sub(pattern, repr(analyzed_image_text), edited_page_content, count=1)\n",
    "                    else:\n",
    "                        edited_page_content += \"\\n\\n\" + analyzed_image_text\n",
    "\n",
    "            edited_pages.append({\n",
    "                \"pageNumber\": page.get(\"pageNumber\"),\n",
    "                \"pageContent\": edited_page_content\n",
    "            })\n",
    "\n",
    "        analyzed_document[\"edited_text\"] = edited_pages\n",
    "        return analyzed_document\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "        return analyzed_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_table_of_image_descriptions(analyzed_document,file) -> list:\n",
    "    image_pathes = []\n",
    "    path = os.path.join(output_raw_document_path,file.get(\"path_wo_origin\"),file.get(\"name_wo_extension\"))\n",
    "    rel_path = os.path.join(file.get(\"path_wo_origin\"),file.get(\"name_wo_extension\"))\n",
    "    for file in glob.glob(f\"{path}\\\\*.png\"):\n",
    "        image_id = file.split(\"\\\\\")[-1].split(\".\")[0].replace(\".\", \"_\")\n",
    "        image_pathes.append({\"name\": image_id, \"path\": file, \"relative_path\": os.path.join(destination_folder,rel_path,image_id+\".png\"), \"pageNumber\": image_id.split(\"_\")[0]})\n",
    "        analyzed_document['analyzed_images'] = image_pathes\n",
    "    \n",
    "    if analyzed_document.get(\"analyzed_images\"):\n",
    "        for image_path in analyzed_document.get(\"analyzed_images\"):\n",
    "            print(image_path)\n",
    "            image_description = create_image_description(image_path.get(\"path\"))\n",
    "            image_path[\"image_description\"] = image_description\n",
    "            image_path[\"type\"] = \"image\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_document(analyzed_document):\n",
    "    fileContent = []\n",
    "    for idx, page in enumerate(analyzed_document.get(\"result\").get(\"pages\")):\n",
    "        pageContent = {\n",
    "            \"pageNumber\": str(analyzed_document.get(\"result\").get(\"pages\")[idx].get(\"pageNumber\")),\n",
    "            \"pageContent\": ' '.join([pageLineContent.get(\"content\") for pageLineContent in page.get(\"lines\") if pageLineContent is not None])\n",
    "        }\n",
    "        fileContent.append(pageContent.copy())\n",
    "\n",
    "    analyzed_document[\"split_document\"] = fileContent\n",
    "\n",
    "    return analyzed_document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document(analyzed_document):\n",
    "    splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=2048,  # Maximum size of each chunk\n",
    "        chunk_overlap=204  # Overlap between chunks\n",
    "    )\n",
    "    chunks_final_list = []\n",
    "    for page in analyzed_document.get(\"edited_text\"):\n",
    "        chunks = splitter.split_text(page.get(\"pageContent\"))\n",
    "        for chunk in chunks:\n",
    "            chunk_result = {\n",
    "            \"pageNumber\": page.get(\"pageNumber\"),\n",
    "            \"pageContent\": chunk\n",
    "            }\n",
    "            chunks_final_list.append(chunk_result.copy())\n",
    "    \n",
    "    analyzed_document[\"chunks\"] = chunks_final_list\n",
    "    return analyzed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_embeddings(analyzed_document):\n",
    "    embeddings = []\n",
    "    for chunk in analyzed_document.get(\"chunks\"):\n",
    "        embeddings.append(client.embeddings.create(\n",
    "            model=\"text-embedding-3-large\",\n",
    "            input=chunk.get(\"pageContent\")\n",
    "        ).data[0].embedding)\n",
    "    analyzed_document['chunk_embedding'] = list(zip(analyzed_document.get(\"chunks\"),embeddings))\n",
    "    return analyzed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_analyzed_document_to_file(analzyedDocument,file,filename):\n",
    "        output_path = os.path.join(output_raw_document_path,file.get(\"path_wo_origin\"),file.get(\"name_wo_extension\"))\n",
    "        create_output_directory(output_path)\n",
    "        with open(os.path.join(output_path,filename), \"w\") as file:\n",
    "            file.write(json.dumps(analzyedDocument))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_uuid_from_string(val: str) -> str:\n",
    "    hex_string = hashlib.md5(val.encode(\"UTF-8\")).hexdigest()\n",
    "    return str(uuid.UUID(hex=hex_string))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_aisearch_output(analyzed_document,file):\n",
    "    list_of_dict=[]\n",
    "    if analyzed_document.get(\"chunk_embedding\"):\n",
    "        for chunk in analyzed_document.get(\"chunk_embedding\"):\n",
    "            unique_id = create_uuid_from_string(chunk[0].get(\"pageContent\"))\n",
    "            chunk_data = {\n",
    "                    \"id\": unique_id,\n",
    "                    \"filename\": file.get(\"name_wo_extension\"),\n",
    "                    \"path\": os.path.join(destination_folder,file.get(\"relative_path\")),\n",
    "                    \"content\": chunk[0][\"pageContent\"],\n",
    "                    \"pageNumber\": chunk[0][\"pageNumber\"],\n",
    "                    \"contentVector\": chunk[1],\n",
    "                    \"type\": \"text\"\n",
    "                }\n",
    "            chunk_data.update(analyzed_document.get(\"metadata\"))\n",
    "\n",
    "            list_of_dict.append(chunk_data)\n",
    "    \n",
    "    if analyzed_document.get(\"analyzed_images\"):\n",
    "        for chunk in analyzed_document.get(\"analyzed_images\"):\n",
    "            unique_id = create_uuid_from_string(chunk.get(\"image_description\"))\n",
    "            image_data = {\n",
    "                    \"id\": unique_id,\n",
    "                    \"filename\": file.get(\"name_wo_extension\"),\n",
    "                    \"path\": chunk.get(\"relative_path\"),\n",
    "                    \"content\": chunk.get(\"image_description\"),\n",
    "                    \"pageNumber\": chunk.get(\"pageNumber\"),\n",
    "                    \"contentVector\": client.embeddings.create(\n",
    "                        model=\"text-embedding-3-large\",\n",
    "                        input=chunk.get(\"image_description\")\n",
    "                    ).data[0].embedding,\n",
    "                    \"type\": chunk.get(\"type\")\n",
    "                }\n",
    "            image_data.update(analyzed_document.get(\"metadata\"))\n",
    "            \n",
    "            list_of_dict.append(image_data)\n",
    "\n",
    "    analyzed_document[\"indexed_data\"] = list_of_dict\n",
    "    \n",
    "    return list_of_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_documents_to_vectordb(list_documents_to_upload):\n",
    "    for doc in list_documents_to_upload:\n",
    "        aiseach_client.upload_documents(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_documents_from_vectordb(file):\n",
    "    if file.get(\"name_wo_extension\"):\n",
    "        results = [res for res in aiseach_client.search(filter=f\"filename eq '{file.get('name_wo_extension')}'\")]\n",
    "        if results:\n",
    "            aiseach_client.delete_documents(documents=results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_document_content(file,prompt,analyzed_document,list_of_sources):\n",
    "    response = client.chat.completions.create(\n",
    "            model='gpt-4o',\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\",\n",
    "                    \"content\": f\"fileObject: {file}, documentContent: {analyzed_document.get('result').get('content')}, list_of_source_files: {list_of_sources}\"\n",
    "                },\n",
    "            ],\n",
    "            temperature=0.0,\n",
    "            response_format={ \"type\": \"json_object\" }\n",
    "        )\n",
    "    analyzed_document[\"metadata\"] = json.loads(response.choices[0].message.content)\n",
    "    return analyzed_document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classification_prompt = \\\n",
    "\"\"\"\n",
    "Your task is to create metadata for a given document and output them as a json object.\n",
    "The document content is based on aviation knowledge base records.\n",
    "\n",
    "\n",
    "1. You match the given document path to a shelf and a book.\n",
    "try to extract from the absolutePath of the file object the shelf and book.\n",
    "The list of shelfs: [\"Welcome\", \"Airport Pilotbriefings\", \"ATC Training (German)\", \"ATC Training (English)\", \"Centersectors\", \"Flight Information Regions (FIRs)\", \"LoA\", \"Pilots\", \"Software\", \"SOPs-Airports\", \"Tools\"]\n",
    "The list of books: [\"AFIS\", \"Air law\", \"Aircraft Knowledge\", \"Airfields Germany\", \"Airports Bremen FIR - EDWW\", \"Airports Langen FIR - EDGG\", \"Airports München FIR - EDMM\", \"Airspace Germany\", \"Allgemein (deutsch)\", \"Ansprechpartner\", \"ATC\", \"ATC English\", \"ATC Software\", \"Aufgaben und Zuständigkeitsbereiche\", \"Ausbildungsübersicht PTD\", \"Bremen FIR (EDWW)\", \"Coordination\", \"CPDLC Logon Codes\", \"EDGG - Langen Radar\", \"EDMM - München Radar\", \"EDUU - Rhein Radar\", \"EDWW - Bremen Radar\", \"EDYY - Maastricht Radar\", \"Einstieg als Pilot\", \"Familiarisation\", \"FIS - Langen Information\", \"Flugzeugkunde\", \"General (englisch)\", \"Heli-Ops\", \"IFR\", \"Koordination\", \"Langen FIR (EDGG)\", \"Luftrecht\", \"Meteorologie\", \"Meteorology\", \"Military\", \"Military Procedures\", \"München FIR (EDMM)\", \"Phraseologie\", \"Phraseology\", \"Pilot\", \"Pilot Software\", \"Practical Procedures\", \"Praktische Verfahren\", \"Quicksheets\", \"Segelflug\", \"Separation\", \"SOPs FIR Bremen\", \"SOPs FIR Langen\", \"SOPs FIR München\", \"Staffelung\", \"Tasks and areas of responsibility\", \"Technical Knowledge\", \"Technikkunde\", \"Trainingsmodule PTD\", \"vACDM\", \"VatGer Touren\", \"VFR\", \"vSID Plugin\", \"Euroscope SID assignment\"]\n",
    "\n",
    "example: \n",
    "path: 'c:\\repositories\\debug\\gptsamples\\vatsim\\output_raw_documents\\willkommen\\pilot.pdf'\n",
    "result: shelf: welcome, book: pilot\n",
    "\n",
    "Shelf and book can only contain exactly one matching item!\n",
    "\n",
    "<<fileObject>>\n",
    "\n",
    "2. Assign keywords based on the document content.\n",
    "Here is a list of keywords: [Controller Information, Pilot Information, General Aviation, VATSIM Information, Phraseology, VFR, IFR]\n",
    "\n",
    "Assign maximum 3 keywords for each document.\n",
    "\n",
    "<<documentContent>>\n",
    "\n",
    "3. Create an abstract from the document content.\n",
    "Not longer than 20 sentences. Try to highlight the key elements of the content.\n",
    "<<documentContent>>\n",
    "\n",
    "Respond with a JSON Object. Do not respond with something else! \n",
    "Output in the following format:\n",
    "\n",
    "{\n",
    "    \"shelf\": shelf\n",
    "    \"book\": book\n",
    "    \"keywords\": [item1, item2, ..]\n",
    "    \"abstract\": abstract\n",
    "}\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_documents_to_upload = []\n",
    "list_of_pdfs = list_files_glob(source_path)\n",
    "for file in list_of_pdfs:\n",
    "    with open(os.path.join(output_raw_document_path,\"uploaded_documents.json\"), \"r\") as f:\n",
    "        files = json.loads(f.read())\n",
    "    if not file.get(\"file_name\") in files:\n",
    "        if not os.path.exists(os.path.join(output_raw_document_path,file.get(\"path_wo_origin\"),file.get(\"name_wo_extension\"),\"analyzed_document.json\")):\n",
    "            analyzed_document = analyze_document(open(file.get(\"absolute_path\"), \"rb\").read())\n",
    "            save_analyzed_document_to_file(analyzed_document,file,\"analyzed_document.json\")\n",
    "            download_images_from_analyzed_document(analyzed_document, file)\n",
    "        else:\n",
    "            analyzed_document = json.loads(open(os.path.join(output_raw_document_path,file.get(\"path_wo_origin\"),file.get(\"name_wo_extension\"),\"analyzed_document.json\"), \"r\").read())\n",
    "        create_table_of_image_descriptions(analyzed_document,file)\n",
    "        split_document(analyzed_document)\n",
    "        replace_image_with_generated_text(analyzed_document)\n",
    "        chunk_document(analyzed_document)\n",
    "        create_embeddings(analyzed_document)\n",
    "        classify_document_content(file,classification_prompt,analyzed_document,list_of_pdfs)\n",
    "        generate_aisearch_output(analyzed_document,file)\n",
    "        save_analyzed_document_to_file(analyzed_document,file,\"enriched_document.json\")\n",
    "        delete_documents_from_vectordb(file)\n",
    "        upload_documents_to_vectordb(analyzed_document.get(\"indexed_data\"))\n",
    "        files.append(file.get(\"file_name\"))\n",
    "        with open(os.path.join(output_raw_document_path,\"uploaded_documents.json\"), \"w\") as f:\n",
    "           f.write(json.dumps(files))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### EXPLORATION TO BE DELETED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# analyzed_document[\"result\"]['tables']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# panda_tables = []\n",
    "# def create_pandas_tables(table):\n",
    "#     column_length = [i for i in range(table.column_count)]\n",
    "#     column_headers = []\n",
    "#     for cell in table.cells:\n",
    "#         if cell.row_index == 0 and cell.get(\"columnSpan\") is None:\n",
    "#             column_headers.append(cell.content)\n",
    "#         elif cell.row_index == 0 and cell.get(\"columnSpan\") is not None:\n",
    "#             column_headers.append(cell.content)\n",
    "#             for i in range(1,cell.get(\"columnSpan\")):\n",
    "#                 column_headers.append(cell.content)\n",
    "    \n",
    "#     df = pd.DataFrame(columns=column_length)\n",
    "#     for cell in table.cells:\n",
    "#         if cell.row_index != 0 and cell.get(\"columnSpan\") is None:\n",
    "#             df.at[cell.row_index,cell.column_index] = cell.content\n",
    "#         elif cell.row_index != 0 and cell.get(\"columnSpan\") is not None:\n",
    "#             df.at[cell.row_index,cell.column_index] = cell.content\n",
    "#             for i in range(1,cell.get(\"columnSpan\")):\n",
    "#                 df.at[cell.row_index,cell.column_index+i] = cell.content\n",
    "\n",
    "#     df.fillna(\"\", inplace=True)\n",
    "#     df.columns = column_headers\n",
    "#     return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_output = (create_pandas_tables(analyzed_document[\"result\"]['tables'][4]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# table_output.to_markdown()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
